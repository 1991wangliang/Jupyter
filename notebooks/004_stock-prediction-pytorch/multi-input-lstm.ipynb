{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "11d1319d-8764-4c29-98e5-f3343ad2754c",
   "metadata": {},
   "source": [
    "# Multi-input LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c79c3b71-83cf-4bc7-b426-affe9cad53ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "# multivariate data preparation\n",
    "from numpy import array\n",
    "from numpy import hstack\n",
    " \n",
    "# split a multivariate sequence into samples\n",
    "def split_sequences(sequences, n_steps):\n",
    "    X, y = list(), list()\n",
    "    for i in range(len(sequences)):\n",
    "        # find the end of this pattern\n",
    "        end_ix = i + n_steps\n",
    "        # check if we are beyond the dataset\n",
    "        if end_ix > len(sequences):\n",
    "            break\n",
    "        # gather input and output parts of the pattern\n",
    "        seq_x, seq_y = sequences[i:end_ix, :-1], sequences[end_ix-1, -1]\n",
    "        X.append(seq_x)\n",
    "        y.append(seq_y)\n",
    "    return array(X), array(y)\n",
    " \n",
    "# define input sequence\n",
    "in_seq1 = array([x for x in range(0,100,10)])\n",
    "in_seq2 = array([x for x in range(5,105,10)])\n",
    "out_seq = array([in_seq1[i]+in_seq2[i] for i in range(len(in_seq1))])\n",
    "# convert to [rows, columns] structure\n",
    "in_seq1 = in_seq1.reshape((len(in_seq1), 1))\n",
    "in_seq2 = in_seq2.reshape((len(in_seq2), 1))\n",
    "out_seq = out_seq.reshape((len(out_seq), 1))\n",
    "# horizontally stack columns\n",
    "dataset = hstack((in_seq1, in_seq2, out_seq))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ff329549-1f3c-4f85-bd81-4152a2d52dec",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MV_LSTM(torch.nn.Module):\n",
    "    def __init__(self,n_features,seq_length):\n",
    "        super(MV_LSTM, self).__init__()\n",
    "        self.n_features = n_features\n",
    "        self.seq_len = seq_length\n",
    "        self.n_hidden = 20 # number of hidden states\n",
    "        self.n_layers = 1 # number of LSTM layers (stacked)\n",
    "    \n",
    "        self.l_lstm = torch.nn.LSTM(input_size = n_features, \n",
    "                                 hidden_size = self.n_hidden,\n",
    "                                 num_layers = self.n_layers, \n",
    "                                 batch_first = True)\n",
    "        # according to pytorch docs LSTM output is \n",
    "        # (batch_size,seq_len, num_directions * hidden_size)\n",
    "        # when considering batch_first = True\n",
    "        self.l_linear = torch.nn.Linear(self.n_hidden*self.seq_len, 1)\n",
    "        \n",
    "    \n",
    "    def init_hidden(self, batch_size):\n",
    "        # even with batch_first = True this remains same as docs\n",
    "        hidden_state = torch.zeros(self.n_layers,batch_size,self.n_hidden)\n",
    "        cell_state = torch.zeros(self.n_layers,batch_size,self.n_hidden)\n",
    "        self.hidden = (hidden_state, cell_state)\n",
    "    \n",
    "    \n",
    "    def forward(self, x):        \n",
    "        batch_size, seq_len, _ = x.size()\n",
    "        \n",
    "        lstm_out, self.hidden = self.l_lstm(x,self.hidden)\n",
    "        # lstm_out(with batch_first = True) is \n",
    "        # (batch_size,seq_len,num_directions * hidden_size)\n",
    "        # for following linear layer we want to keep batch_size dimension and merge rest       \n",
    "        # .contiguous() -> solves tensor compatibility error\n",
    "        x = lstm_out.contiguous().view(batch_size,-1)\n",
    "        return self.l_linear(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2e964161-ecd1-46bb-afcd-1769098e5033",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8, 3, 2) (8,)\n"
     ]
    }
   ],
   "source": [
    "n_features = 2 # this is number of parallel inputs\n",
    "n_timesteps = 3 # this is number of timesteps\n",
    "\n",
    "# convert dataset into input/output\n",
    "X, y = split_sequences(dataset, n_timesteps)\n",
    "print(X.shape, y.shape)\n",
    "\n",
    "# create NN\n",
    "mv_net = MV_LSTM(n_features,n_timesteps)\n",
    "criterion = torch.nn.MSELoss() # reduction='sum' created huge loss value\n",
    "optimizer = torch.optim.Adam(mv_net.parameters(), lr=1e-1)\n",
    "\n",
    "train_episodes = 500\n",
    "batch_size = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0ae9ee5e-57eb-4349-9d0d-ff0f02ebb8c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step :  0 loss :  15275.109375\n",
      "step :  1 loss :  14860.890625\n",
      "step :  2 loss :  14271.1181640625\n",
      "step :  3 loss :  13561.9931640625\n",
      "step :  4 loss :  12825.68359375\n",
      "step :  5 loss :  12134.0673828125\n",
      "step :  6 loss :  11564.7666015625\n",
      "step :  7 loss :  10820.8681640625\n",
      "step :  8 loss :  10222.8515625\n",
      "step :  9 loss :  9456.4306640625\n",
      "step :  10 loss :  8845.1669921875\n",
      "step :  11 loss :  8302.3544921875\n",
      "step :  12 loss :  7776.43896484375\n",
      "step :  13 loss :  7264.974609375\n",
      "step :  14 loss :  6699.1845703125\n",
      "step :  15 loss :  6289.5458984375\n",
      "step :  16 loss :  5853.923828125\n",
      "step :  17 loss :  5423.52587890625\n",
      "step :  18 loss :  5007.125\n",
      "step :  19 loss :  4660.1044921875\n",
      "step :  20 loss :  4351.806640625\n",
      "step :  21 loss :  4034.1201171875\n",
      "step :  22 loss :  3760.90966796875\n",
      "step :  23 loss :  3512.22607421875\n",
      "step :  24 loss :  3278.61181640625\n",
      "step :  25 loss :  3067.07373046875\n",
      "step :  26 loss :  2881.93408203125\n",
      "step :  27 loss :  2720.51806640625\n",
      "step :  28 loss :  2580.851806640625\n",
      "step :  29 loss :  2461.08447265625\n",
      "step :  30 loss :  2359.2099609375\n",
      "step :  31 loss :  2272.6591796875\n",
      "step :  32 loss :  2197.919921875\n",
      "step :  33 loss :  2130.447021484375\n",
      "step :  34 loss :  2067.841796875\n",
      "step :  35 loss :  1963.66064453125\n",
      "step :  36 loss :  1845.598388671875\n",
      "step :  37 loss :  1703.812744140625\n",
      "step :  38 loss :  1884.534423828125\n",
      "step :  39 loss :  1850.5648193359375\n",
      "step :  40 loss :  1514.470458984375\n",
      "step :  41 loss :  1544.770263671875\n",
      "step :  42 loss :  1611.1611328125\n",
      "step :  43 loss :  1623.8497314453125\n",
      "step :  44 loss :  1618.139892578125\n",
      "step :  45 loss :  1607.09521484375\n",
      "step :  46 loss :  1596.6143798828125\n",
      "step :  47 loss :  1573.4044189453125\n",
      "step :  48 loss :  1526.1466064453125\n",
      "step :  49 loss :  1455.407470703125\n",
      "step :  50 loss :  1388.8134765625\n",
      "step :  51 loss :  1554.283203125\n",
      "step :  52 loss :  1243.744384765625\n",
      "step :  53 loss :  1131.021728515625\n",
      "step :  54 loss :  1047.05859375\n",
      "step :  55 loss :  926.9788818359375\n",
      "step :  56 loss :  898.2108154296875\n",
      "step :  57 loss :  1057.991943359375\n",
      "step :  58 loss :  1355.88037109375\n",
      "step :  59 loss :  971.7418212890625\n",
      "step :  60 loss :  846.82080078125\n",
      "step :  61 loss :  790.05322265625\n",
      "step :  62 loss :  713.6453247070312\n",
      "step :  63 loss :  699.5457763671875\n",
      "step :  64 loss :  818.2064819335938\n",
      "step :  65 loss :  792.39111328125\n",
      "step :  66 loss :  644.4149169921875\n",
      "step :  67 loss :  777.4278564453125\n",
      "step :  68 loss :  767.3651123046875\n",
      "step :  69 loss :  790.7437744140625\n",
      "step :  70 loss :  770.0992431640625\n",
      "step :  71 loss :  711.609619140625\n",
      "step :  72 loss :  806.2612915039062\n",
      "step :  73 loss :  698.703857421875\n",
      "step :  74 loss :  730.5755615234375\n",
      "step :  75 loss :  728.063720703125\n",
      "step :  76 loss :  705.6822509765625\n",
      "step :  77 loss :  674.3880615234375\n",
      "step :  78 loss :  624.007080078125\n",
      "step :  79 loss :  567.54833984375\n",
      "step :  80 loss :  548.734375\n",
      "step :  81 loss :  520.9067993164062\n",
      "step :  82 loss :  489.64697265625\n",
      "step :  83 loss :  488.5628967285156\n",
      "step :  84 loss :  521.6380615234375\n",
      "step :  85 loss :  444.4014892578125\n",
      "step :  86 loss :  436.27569580078125\n",
      "step :  87 loss :  442.29888916015625\n",
      "step :  88 loss :  430.92578125\n",
      "step :  89 loss :  404.66912841796875\n",
      "step :  90 loss :  375.32879638671875\n",
      "step :  91 loss :  331.37554931640625\n",
      "step :  92 loss :  292.58099365234375\n",
      "step :  93 loss :  276.498046875\n",
      "step :  94 loss :  293.2986145019531\n",
      "step :  95 loss :  279.2327880859375\n",
      "step :  96 loss :  264.8838806152344\n",
      "step :  97 loss :  247.936767578125\n",
      "step :  98 loss :  238.33128356933594\n",
      "step :  99 loss :  241.07254028320312\n",
      "step :  100 loss :  233.80453491210938\n",
      "step :  101 loss :  225.6944580078125\n",
      "step :  102 loss :  212.60499572753906\n",
      "step :  103 loss :  209.00120544433594\n",
      "step :  104 loss :  201.09161376953125\n",
      "step :  105 loss :  197.64089965820312\n",
      "step :  106 loss :  196.81781005859375\n",
      "step :  107 loss :  191.5578155517578\n",
      "step :  108 loss :  185.53369140625\n",
      "step :  109 loss :  181.04669189453125\n",
      "step :  110 loss :  177.69198608398438\n",
      "step :  111 loss :  173.4737548828125\n",
      "step :  112 loss :  168.27664184570312\n",
      "step :  113 loss :  164.1909637451172\n",
      "step :  114 loss :  161.29928588867188\n",
      "step :  115 loss :  157.72808837890625\n",
      "step :  116 loss :  155.52633666992188\n",
      "step :  117 loss :  153.87298583984375\n",
      "step :  118 loss :  150.20486450195312\n",
      "step :  119 loss :  148.03964233398438\n",
      "step :  120 loss :  146.2687225341797\n",
      "step :  121 loss :  143.5615234375\n",
      "step :  122 loss :  140.5589599609375\n",
      "step :  123 loss :  138.25531005859375\n",
      "step :  124 loss :  136.31161499023438\n",
      "step :  125 loss :  133.98983764648438\n",
      "step :  126 loss :  131.79632568359375\n",
      "step :  127 loss :  129.7392578125\n",
      "step :  128 loss :  127.78206634521484\n",
      "step :  129 loss :  126.15292358398438\n",
      "step :  130 loss :  124.842529296875\n",
      "step :  131 loss :  123.55609893798828\n",
      "step :  132 loss :  122.11176300048828\n",
      "step :  133 loss :  120.51729583740234\n",
      "step :  134 loss :  118.84713745117188\n",
      "step :  135 loss :  117.22935485839844\n",
      "step :  136 loss :  115.80534362792969\n",
      "step :  137 loss :  114.59140014648438\n",
      "step :  138 loss :  113.5006103515625\n",
      "step :  139 loss :  112.40922546386719\n",
      "step :  140 loss :  111.29132843017578\n",
      "step :  141 loss :  110.19554138183594\n",
      "step :  142 loss :  109.11921691894531\n",
      "step :  143 loss :  108.03872680664062\n",
      "step :  144 loss :  106.93801879882812\n",
      "step :  145 loss :  105.86831665039062\n",
      "step :  146 loss :  104.89337921142578\n",
      "step :  147 loss :  103.99850463867188\n",
      "step :  148 loss :  103.13308715820312\n",
      "step :  149 loss :  102.25918579101562\n",
      "step :  150 loss :  101.37752532958984\n",
      "step :  151 loss :  100.51258850097656\n",
      "step :  152 loss :  99.66748046875\n",
      "step :  153 loss :  98.82598876953125\n",
      "step :  154 loss :  97.98754119873047\n",
      "step :  155 loss :  97.16617584228516\n",
      "step :  156 loss :  96.37574005126953\n",
      "step :  157 loss :  95.61518859863281\n",
      "step :  158 loss :  94.86592102050781\n",
      "step :  159 loss :  94.11518859863281\n",
      "step :  160 loss :  93.36679077148438\n",
      "step :  161 loss :  92.63004302978516\n",
      "step :  162 loss :  91.90911865234375\n",
      "step :  163 loss :  91.19690704345703\n",
      "step :  164 loss :  90.48774719238281\n",
      "step :  165 loss :  89.78347778320312\n",
      "step :  166 loss :  89.08870697021484\n",
      "step :  167 loss :  88.40567016601562\n",
      "step :  168 loss :  87.7281494140625\n",
      "step :  169 loss :  87.04985046386719\n",
      "step :  170 loss :  86.36978149414062\n",
      "step :  171 loss :  85.69255065917969\n",
      "step :  172 loss :  85.02503967285156\n",
      "step :  173 loss :  84.37460327148438\n",
      "step :  174 loss :  83.740478515625\n",
      "step :  175 loss :  83.10614013671875\n",
      "step :  176 loss :  82.46488189697266\n",
      "step :  177 loss :  81.83586120605469\n",
      "step :  178 loss :  81.22108459472656\n",
      "step :  179 loss :  80.60404968261719\n",
      "step :  180 loss :  79.98049926757812\n",
      "step :  181 loss :  79.36273193359375\n",
      "step :  182 loss :  78.7606201171875\n",
      "step :  183 loss :  78.16790008544922\n",
      "step :  184 loss :  77.57440948486328\n",
      "step :  185 loss :  76.98017883300781\n",
      "step :  186 loss :  76.38737487792969\n",
      "step :  187 loss :  75.79854583740234\n",
      "step :  188 loss :  75.21621704101562\n",
      "step :  189 loss :  74.63700866699219\n",
      "step :  190 loss :  74.0559310913086\n",
      "step :  191 loss :  73.47364044189453\n",
      "step :  192 loss :  72.89154052734375\n",
      "step :  193 loss :  72.3030014038086\n",
      "step :  194 loss :  71.70196533203125\n",
      "step :  195 loss :  71.08673095703125\n",
      "step :  196 loss :  70.44804382324219\n",
      "step :  197 loss :  69.76321411132812\n",
      "step :  198 loss :  68.99654388427734\n",
      "step :  199 loss :  68.07963562011719\n",
      "step :  200 loss :  66.86995697021484\n",
      "step :  201 loss :  65.07562255859375\n",
      "step :  202 loss :  62.244598388671875\n",
      "step :  203 loss :  58.55436325073242\n",
      "step :  204 loss :  55.700660705566406\n",
      "step :  205 loss :  52.991172790527344\n",
      "step :  206 loss :  44.710777282714844\n",
      "step :  207 loss :  64.49348449707031\n",
      "step :  208 loss :  45.571441650390625\n",
      "step :  209 loss :  57.29133605957031\n",
      "step :  210 loss :  55.995357513427734\n",
      "step :  211 loss :  54.06678009033203\n",
      "step :  212 loss :  55.79808044433594\n",
      "step :  213 loss :  54.33897399902344\n",
      "step :  214 loss :  53.53118896484375\n",
      "step :  215 loss :  53.33428955078125\n",
      "step :  216 loss :  49.85505676269531\n",
      "step :  217 loss :  43.147613525390625\n",
      "step :  218 loss :  38.34333038330078\n",
      "step :  219 loss :  31.955564498901367\n",
      "step :  220 loss :  35.418907165527344\n",
      "step :  221 loss :  45.07026672363281\n",
      "step :  222 loss :  56.28437042236328\n",
      "step :  223 loss :  46.284446716308594\n",
      "step :  224 loss :  34.15998077392578\n",
      "step :  225 loss :  31.259458541870117\n",
      "step :  226 loss :  28.609695434570312\n",
      "step :  227 loss :  32.64836883544922\n",
      "step :  228 loss :  31.584867477416992\n",
      "step :  229 loss :  32.29991912841797\n",
      "step :  230 loss :  31.86133575439453\n",
      "step :  231 loss :  27.588098526000977\n",
      "step :  232 loss :  29.42620849609375\n",
      "step :  233 loss :  28.356361389160156\n",
      "step :  234 loss :  26.42993927001953\n",
      "step :  235 loss :  31.053260803222656\n",
      "step :  236 loss :  57.21905517578125\n",
      "step :  237 loss :  58.446083068847656\n",
      "step :  238 loss :  44.34210205078125\n",
      "step :  239 loss :  27.5728759765625\n",
      "step :  240 loss :  47.1802864074707\n",
      "step :  241 loss :  30.560272216796875\n",
      "step :  242 loss :  24.39327621459961\n",
      "step :  243 loss :  38.758453369140625\n",
      "step :  244 loss :  22.603580474853516\n",
      "step :  245 loss :  36.54277801513672\n",
      "step :  246 loss :  28.892257690429688\n",
      "step :  247 loss :  27.842679977416992\n",
      "step :  248 loss :  32.42198944091797\n",
      "step :  249 loss :  21.791528701782227\n",
      "step :  250 loss :  23.77227020263672\n",
      "step :  251 loss :  32.34095001220703\n",
      "step :  252 loss :  26.488910675048828\n",
      "step :  253 loss :  39.493682861328125\n",
      "step :  254 loss :  28.18914794921875\n",
      "step :  255 loss :  25.221149444580078\n",
      "step :  256 loss :  24.034191131591797\n",
      "step :  257 loss :  25.91653823852539\n",
      "step :  258 loss :  20.35019302368164\n",
      "step :  259 loss :  21.80877685546875\n",
      "step :  260 loss :  25.446399688720703\n",
      "step :  261 loss :  20.06709098815918\n",
      "step :  262 loss :  20.095806121826172\n",
      "step :  263 loss :  23.515335083007812\n",
      "step :  264 loss :  22.359737396240234\n",
      "step :  265 loss :  20.832557678222656\n",
      "step :  266 loss :  20.975635528564453\n",
      "step :  267 loss :  18.590824127197266\n",
      "step :  268 loss :  16.933008193969727\n",
      "step :  269 loss :  18.55019187927246\n",
      "step :  270 loss :  18.63886070251465\n",
      "step :  271 loss :  17.033811569213867\n",
      "step :  272 loss :  17.032794952392578\n",
      "step :  273 loss :  17.473731994628906\n",
      "step :  274 loss :  15.934247970581055\n",
      "step :  275 loss :  15.635353088378906\n",
      "step :  276 loss :  15.992292404174805\n",
      "step :  277 loss :  15.905980110168457\n",
      "step :  278 loss :  14.849357604980469\n",
      "step :  279 loss :  15.471553802490234\n",
      "step :  280 loss :  15.84467887878418\n",
      "step :  281 loss :  14.553448677062988\n",
      "step :  282 loss :  14.1558837890625\n",
      "step :  283 loss :  14.508731842041016\n",
      "step :  284 loss :  13.636846542358398\n",
      "step :  285 loss :  13.026632308959961\n",
      "step :  286 loss :  13.577201843261719\n",
      "step :  287 loss :  14.088005065917969\n",
      "step :  288 loss :  13.073695182800293\n",
      "step :  289 loss :  12.539281845092773\n",
      "step :  290 loss :  12.91645622253418\n",
      "step :  291 loss :  13.096534729003906\n",
      "step :  292 loss :  12.611074447631836\n",
      "step :  293 loss :  12.113298416137695\n",
      "step :  294 loss :  12.103950500488281\n",
      "step :  295 loss :  12.275968551635742\n",
      "step :  296 loss :  12.005112648010254\n",
      "step :  297 loss :  11.624852180480957\n",
      "step :  298 loss :  11.436031341552734\n",
      "step :  299 loss :  11.466756820678711\n",
      "step :  300 loss :  11.429145812988281\n",
      "step :  301 loss :  11.22882080078125\n",
      "step :  302 loss :  10.982192993164062\n",
      "step :  303 loss :  10.81091022491455\n",
      "step :  304 loss :  10.748126029968262\n",
      "step :  305 loss :  10.721868515014648\n",
      "step :  306 loss :  10.624744415283203\n",
      "step :  307 loss :  10.425342559814453\n",
      "step :  308 loss :  10.215396881103516\n",
      "step :  309 loss :  10.086048126220703\n",
      "step :  310 loss :  10.028267860412598\n",
      "step :  311 loss :  9.980205535888672\n",
      "step :  312 loss :  9.897567749023438\n",
      "step :  313 loss :  9.770431518554688\n",
      "step :  314 loss :  9.614909172058105\n",
      "step :  315 loss :  9.478894233703613\n",
      "step :  316 loss :  9.376509666442871\n",
      "step :  317 loss :  9.288894653320312\n",
      "step :  318 loss :  9.201559066772461\n",
      "step :  319 loss :  9.12004566192627\n",
      "step :  320 loss :  9.04680061340332\n",
      "step :  321 loss :  8.961517333984375\n",
      "step :  322 loss :  8.856277465820312\n",
      "step :  323 loss :  8.737865447998047\n",
      "step :  324 loss :  8.626073837280273\n",
      "step :  325 loss :  8.524856567382812\n",
      "step :  326 loss :  8.43217945098877\n",
      "step :  327 loss :  8.343146324157715\n",
      "step :  328 loss :  8.254597663879395\n",
      "step :  329 loss :  8.169618606567383\n",
      "step :  330 loss :  8.090875625610352\n",
      "step :  331 loss :  8.014129638671875\n",
      "step :  332 loss :  7.932799339294434\n",
      "step :  333 loss :  7.8503265380859375\n",
      "step :  334 loss :  7.7688307762146\n",
      "step :  335 loss :  7.689737319946289\n",
      "step :  336 loss :  7.6065673828125\n",
      "step :  337 loss :  7.523187637329102\n",
      "step :  338 loss :  7.439226150512695\n",
      "step :  339 loss :  7.358660697937012\n",
      "step :  340 loss :  7.278083324432373\n",
      "step :  341 loss :  7.199190616607666\n",
      "step :  342 loss :  7.117794513702393\n",
      "step :  343 loss :  7.038549423217773\n",
      "step :  344 loss :  6.960735321044922\n",
      "step :  345 loss :  6.8860087394714355\n",
      "step :  346 loss :  6.811282157897949\n",
      "step :  347 loss :  6.740129470825195\n",
      "step :  348 loss :  6.670479774475098\n",
      "step :  349 loss :  6.606156349182129\n",
      "step :  350 loss :  6.541119575500488\n",
      "step :  351 loss :  6.483931541442871\n",
      "step :  352 loss :  6.426152229309082\n",
      "step :  353 loss :  6.384375095367432\n",
      "step :  354 loss :  6.338665008544922\n",
      "step :  355 loss :  6.321413516998291\n",
      "step :  356 loss :  6.282759189605713\n",
      "step :  357 loss :  6.290822505950928\n",
      "step :  358 loss :  6.233430862426758\n",
      "step :  359 loss :  6.234879493713379\n",
      "step :  360 loss :  6.10840368270874\n",
      "step :  361 loss :  6.035493850708008\n",
      "step :  362 loss :  5.85896110534668\n",
      "step :  363 loss :  5.73019552230835\n",
      "step :  364 loss :  5.599260330200195\n",
      "step :  365 loss :  5.501965045928955\n",
      "step :  366 loss :  5.423254013061523\n",
      "step :  367 loss :  5.35689115524292\n",
      "step :  368 loss :  5.295897483825684\n",
      "step :  369 loss :  5.236820697784424\n",
      "step :  370 loss :  5.180034160614014\n",
      "step :  371 loss :  5.128902912139893\n",
      "step :  372 loss :  5.090361595153809\n",
      "step :  373 loss :  5.070647716522217\n",
      "step :  374 loss :  5.095797538757324\n",
      "step :  375 loss :  5.133541584014893\n",
      "step :  376 loss :  5.2870330810546875\n",
      "step :  377 loss :  5.279145240783691\n",
      "step :  378 loss :  5.421342372894287\n",
      "step :  379 loss :  5.0833892822265625\n",
      "step :  380 loss :  4.854292392730713\n",
      "step :  381 loss :  4.6067118644714355\n",
      "step :  382 loss :  4.544012069702148\n",
      "step :  383 loss :  4.6507649421691895\n",
      "step :  384 loss :  4.873958587646484\n",
      "step :  385 loss :  5.5472636222839355\n",
      "step :  386 loss :  5.790395259857178\n",
      "step :  387 loss :  6.483759880065918\n",
      "step :  388 loss :  4.683889389038086\n",
      "step :  389 loss :  4.708251476287842\n",
      "step :  390 loss :  5.5792236328125\n",
      "step :  391 loss :  4.402375221252441\n",
      "step :  392 loss :  4.301045894622803\n",
      "step :  393 loss :  5.378296375274658\n",
      "step :  394 loss :  5.06263542175293\n",
      "step :  395 loss :  4.980944633483887\n",
      "step :  396 loss :  4.034175395965576\n",
      "step :  397 loss :  3.9281105995178223\n",
      "step :  398 loss :  4.378655910491943\n",
      "step :  399 loss :  4.206307411193848\n",
      "step :  400 loss :  3.9451801776885986\n",
      "step :  401 loss :  3.657243013381958\n",
      "step :  402 loss :  3.8011481761932373\n",
      "step :  403 loss :  4.4173479080200195\n",
      "step :  404 loss :  4.656232833862305\n",
      "step :  405 loss :  5.53330659866333\n",
      "step :  406 loss :  4.053910255432129\n",
      "step :  407 loss :  3.569410800933838\n",
      "step :  408 loss :  4.107694625854492\n",
      "step :  409 loss :  3.7726829051971436\n",
      "step :  410 loss :  3.342390775680542\n",
      "step :  411 loss :  3.2891788482666016\n",
      "step :  412 loss :  3.6177337169647217\n",
      "step :  413 loss :  4.498979091644287\n",
      "step :  414 loss :  4.762020587921143\n",
      "step :  415 loss :  5.870408535003662\n",
      "step :  416 loss :  3.554643154144287\n",
      "step :  417 loss :  4.035740852355957\n",
      "step :  418 loss :  4.783326148986816\n",
      "step :  419 loss :  3.0732221603393555\n",
      "step :  420 loss :  4.667046070098877\n",
      "step :  421 loss :  8.407835960388184\n",
      "step :  422 loss :  3.3918166160583496\n",
      "step :  423 loss :  6.257061004638672\n",
      "step :  424 loss :  8.998710632324219\n",
      "step :  425 loss :  3.1810193061828613\n",
      "step :  426 loss :  6.209519386291504\n",
      "step :  427 loss :  18.162174224853516\n",
      "step :  428 loss :  5.501509189605713\n",
      "step :  429 loss :  10.059269905090332\n",
      "step :  430 loss :  13.527261734008789\n",
      "step :  431 loss :  10.494446754455566\n",
      "step :  432 loss :  4.79279899597168\n",
      "step :  433 loss :  16.325361251831055\n",
      "step :  434 loss :  5.892240524291992\n",
      "step :  435 loss :  9.759770393371582\n",
      "step :  436 loss :  6.664601802825928\n",
      "step :  437 loss :  13.588443756103516\n",
      "step :  438 loss :  21.42320442199707\n",
      "step :  439 loss :  24.935300827026367\n",
      "step :  440 loss :  3.8556110858917236\n",
      "step :  441 loss :  11.228595733642578\n",
      "step :  442 loss :  7.354903697967529\n",
      "step :  443 loss :  11.684236526489258\n",
      "step :  444 loss :  8.29373550415039\n",
      "step :  445 loss :  6.717414855957031\n",
      "step :  446 loss :  16.86043930053711\n",
      "step :  447 loss :  7.3866496086120605\n",
      "step :  448 loss :  12.348722457885742\n",
      "step :  449 loss :  8.081496238708496\n",
      "step :  450 loss :  5.011892318725586\n",
      "step :  451 loss :  5.024827003479004\n",
      "step :  452 loss :  15.603079795837402\n",
      "step :  453 loss :  5.957913398742676\n",
      "step :  454 loss :  7.597251892089844\n",
      "step :  455 loss :  6.902331352233887\n",
      "step :  456 loss :  12.71760082244873\n",
      "step :  457 loss :  3.4595210552215576\n",
      "step :  458 loss :  8.71714973449707\n",
      "step :  459 loss :  11.4934720993042\n",
      "step :  460 loss :  19.914710998535156\n",
      "step :  461 loss :  21.379165649414062\n",
      "step :  462 loss :  6.512886047363281\n",
      "step :  463 loss :  23.56018829345703\n",
      "step :  464 loss :  6.4780097007751465\n",
      "step :  465 loss :  14.002351760864258\n",
      "step :  466 loss :  13.857315063476562\n",
      "step :  467 loss :  15.909088134765625\n",
      "step :  468 loss :  37.96198272705078\n",
      "step :  469 loss :  18.92604637145996\n",
      "step :  470 loss :  29.295543670654297\n",
      "step :  471 loss :  31.35590362548828\n",
      "step :  472 loss :  39.525936126708984\n",
      "step :  473 loss :  45.98011016845703\n",
      "step :  474 loss :  27.109474182128906\n",
      "step :  475 loss :  14.997101783752441\n",
      "step :  476 loss :  33.663963317871094\n",
      "step :  477 loss :  53.50093078613281\n",
      "step :  478 loss :  22.645462036132812\n",
      "step :  479 loss :  39.39594268798828\n",
      "step :  480 loss :  52.35883712768555\n",
      "step :  481 loss :  19.96063804626465\n",
      "step :  482 loss :  31.703237533569336\n",
      "step :  483 loss :  56.419429779052734\n",
      "step :  484 loss :  44.57490921020508\n",
      "step :  485 loss :  42.16144561767578\n",
      "step :  486 loss :  20.610105514526367\n",
      "step :  487 loss :  18.54942512512207\n",
      "step :  488 loss :  33.23041534423828\n",
      "step :  489 loss :  23.263872146606445\n",
      "step :  490 loss :  46.62390899658203\n",
      "step :  491 loss :  20.238630294799805\n",
      "step :  492 loss :  20.339757919311523\n",
      "step :  493 loss :  13.757768630981445\n",
      "step :  494 loss :  12.55864143371582\n",
      "step :  495 loss :  14.749250411987305\n",
      "step :  496 loss :  28.199321746826172\n",
      "step :  497 loss :  5.582187175750732\n",
      "step :  498 loss :  12.496185302734375\n",
      "step :  499 loss :  8.679424285888672\n"
     ]
    }
   ],
   "source": [
    "mv_net.train()\n",
    "for t in range(train_episodes):\n",
    "    for b in range(0,len(X),batch_size):\n",
    "        inpt = X[b:b+batch_size,:,:]\n",
    "        target = y[b:b+batch_size]    \n",
    "        \n",
    "        x_batch = torch.tensor(inpt,dtype=torch.float32)    \n",
    "        y_batch = torch.tensor(target,dtype=torch.float32)\n",
    "    \n",
    "        mv_net.init_hidden(x_batch.size(0))\n",
    "    #    lstm_out, _ = mv_net.l_lstm(x_batch,nnet.hidden)    \n",
    "    #    lstm_out.contiguous().view(x_batch.size(0),-1)\n",
    "        output = mv_net(x_batch) \n",
    "        loss = criterion(output.view(-1), y_batch)  \n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()        \n",
    "        optimizer.zero_grad() \n",
    "    print('step : ' , t , 'loss : ' , loss.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b46d3a9-1a83-4978-b3f3-6b16c3557906",
   "metadata": {},
   "source": [
    "## Link\n",
    "https://stackoverflow.com/questions/56858924/multivariate-input-lstm-in-pytorch  \n",
    "https://machinelearningmastery.com/multivariate-time-series-forecasting-lstms-keras/  \n",
    "https://www.kaggle.com/code/jphoon/bitcoin-time-series-prediction-with-lstm  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c39bf9b2-8121-4064-83a2-4fb001685e78",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
